"use client";

import TopicSection from "@/components/TopicSection";
import CodeBlock from "@/components/CodeBlock";
import { InlineMath, BlockMath } from "@/components/Math";

export default function VariationalAutoencoders() {
  return (
    <div>
      <TopicSection type="plain-english">
        <p>
          A Variational Autoencoder (VAE) is a generative model that learns to <strong>compress data into a
          compact latent representation and then reconstruct it</strong>. Unlike a regular autoencoder, a VAE
          doesn&apos;t just learn a single point in latent space for each input — it learns a <strong>distribution</strong>.
          This is what makes it generative: you can sample from the latent distribution to create entirely new data.
        </p>
        <p>
          Think of it like this: a regular autoencoder memorizes a specific parking spot for each car.
          A VAE instead learns a <strong>neighborhood</strong> for each car, and any spot in that neighborhood
          produces something that looks like that car. Because these neighborhoods overlap smoothly, you can
          sample <em>between</em> them to get novel, realistic outputs — like a car that blends features of
          two different models.
        </p>
        <p>
          The VAE has two networks: an <strong>encoder</strong> (maps input to a distribution over latent space)
          and a <strong>decoder</strong> (maps a latent sample back to data space). Training balances two goals:
          reconstruct the input accurately, and keep the latent distribution close to a standard normal so that
          sampling produces coherent outputs.
        </p>
      </TopicSection>

      <TopicSection type="math">
        <h3>The Generative Story</h3>
        <p>
          We assume data <InlineMath math="x" /> is generated by first sampling a latent variable
          <InlineMath math="z \sim p(z) = \mathcal{N}(0, I)" />, then producing <InlineMath math="x \sim p_\theta(x|z)" />.
          We want to maximize the marginal likelihood:
        </p>
        <BlockMath math="p_\theta(x) = \int p_\theta(x|z)\,p(z)\,dz" />
        <p>
          This integral is intractable because it requires summing over all possible latent codes. We introduce
          an approximate posterior <InlineMath math="q_\phi(z|x)" /> (the encoder) and derive a tractable lower bound.
        </p>

        <h3>Evidence Lower Bound (ELBO)</h3>
        <p>Starting from the log-marginal likelihood:</p>
        <BlockMath math="\log p_\theta(x) = \log \int p_\theta(x|z)\,p(z)\,dz" />
        <p>By Jensen&apos;s inequality (or equivalently, by adding and subtracting <InlineMath math="q_\phi(z|x)" />):</p>
        <BlockMath math="\log p_\theta(x) \geq \underbrace{E_{q_\phi(z|x)}[\log p_\theta(x|z)]}_{\text{Reconstruction}} - \underbrace{D_{KL}(q_\phi(z|x) \| p(z))}_{\text{Regularization}} = \mathcal{L}(\theta, \phi; x)" />
        <p>
          The ELBO has two terms: the <strong>reconstruction term</strong> encourages the decoder to accurately
          reconstruct <InlineMath math="x" /> from <InlineMath math="z" />, while the <strong>KL divergence</strong> term
          regularizes the encoder to stay close to the prior <InlineMath math="p(z) = \mathcal{N}(0, I)" />.
        </p>

        <h3>KL Divergence (Closed Form for Gaussians)</h3>
        <p>When <InlineMath math="q_\phi(z|x) = \mathcal{N}(\mu, \text{diag}(\sigma^2))" /> and <InlineMath math="p(z) = \mathcal{N}(0, I)" />:</p>
        <BlockMath math="D_{KL} = -\frac{1}{2}\sum_{j=1}^{d}\left(1 + \log \sigma_j^2 - \mu_j^2 - \sigma_j^2\right)" />

        <h3>Reparameterization Trick</h3>
        <p>
          We cannot backpropagate through a random sampling operation. Instead, we reparameterize:
        </p>
        <BlockMath math="z = \mu + \sigma \odot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)" />
        <p>
          Now <InlineMath math="z" /> is a deterministic function of <InlineMath math="\mu" />, <InlineMath math="\sigma" />,
          and the noise <InlineMath math="\epsilon" />, so gradients flow through <InlineMath math="\mu" /> and
          <InlineMath math="\sigma" /> as usual.
        </p>
      </TopicSection>

      <TopicSection type="code">
        <h3>VAE from Scratch in PyTorch</h3>
        <CodeBlock
          language="python"
          title="vae.py"
          code={`import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

class VAE(nn.Module):
    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):
        super().__init__()
        # Encoder
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc_mu = nn.Linear(hidden_dim, latent_dim)
        self.fc_logvar = nn.Linear(hidden_dim, latent_dim)
        # Decoder
        self.fc3 = nn.Linear(latent_dim, hidden_dim)
        self.fc4 = nn.Linear(hidden_dim, input_dim)

    def encode(self, x):
        h = F.relu(self.fc1(x))
        return self.fc_mu(h), self.fc_logvar(h)

    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)       # sigma = exp(0.5 * log(sigma^2))
        eps = torch.randn_like(std)          # epsilon ~ N(0, I)
        return mu + std * eps                # z = mu + sigma * epsilon

    def decode(self, z):
        h = F.relu(self.fc3(z))
        return torch.sigmoid(self.fc4(h))

    def forward(self, x):
        mu, logvar = self.encode(x.view(-1, 784))
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar

def vae_loss(recon_x, x, mu, logvar):
    """ELBO loss = reconstruction + KL divergence."""
    # Binary cross-entropy for reconstruction (Bernoulli decoder)
    bce = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')
    # Closed-form KL divergence for Gaussians
    kl = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
    return bce + kl

# Training loop
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = VAE().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)

train_loader = DataLoader(
    datasets.MNIST('./data', train=True, download=True,
                   transform=transforms.ToTensor()),
    batch_size=128, shuffle=True
)

model.train()
for epoch in range(10):
    total_loss = 0
    for batch, _ in train_loader:
        batch = batch.to(device)
        recon, mu, logvar = model(batch)
        loss = vae_loss(recon, batch, mu, logvar)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg = total_loss / len(train_loader.dataset)
    print(f"Epoch {epoch+1:2d} | Avg Loss: {avg:.2f}")`}
        />

        <h3>Generating New Samples</h3>
        <CodeBlock
          language="python"
          title="vae_generate.py"
          code={`import matplotlib.pyplot as plt

model.eval()
with torch.no_grad():
    # Sample from the prior N(0, I)
    z = torch.randn(16, 20).to(device)
    samples = model.decode(z).view(-1, 1, 28, 28).cpu()

fig, axes = plt.subplots(2, 8, figsize=(12, 3))
for i, ax in enumerate(axes.flat):
    ax.imshow(samples[i, 0], cmap='gray')
    ax.axis('off')
plt.suptitle("VAE-Generated MNIST Digits")
plt.tight_layout()
plt.show()

# Latent space interpolation between two digits
z1 = torch.randn(1, 20).to(device)
z2 = torch.randn(1, 20).to(device)
alphas = torch.linspace(0, 1, 10).unsqueeze(1).to(device)
z_interp = z1 * (1 - alphas) + z2 * alphas
decoded = model.decode(z_interp).view(-1, 1, 28, 28).cpu()

fig, axes = plt.subplots(1, 10, figsize=(15, 2))
for i, ax in enumerate(axes):
    ax.imshow(decoded[i, 0], cmap='gray')
    ax.axis('off')
plt.suptitle("Latent Space Interpolation")
plt.show()`}
        />
      </TopicSection>

      <TopicSection type="in-practice">
        <ul>
          <li><strong>Blurry outputs</strong>: Vanilla VAEs produce blurry images because the Gaussian decoder averages over modes. Use a more expressive decoder (convolutional) or switch to VQ-VAE for sharper results.</li>
          <li><strong>Beta-VAE</strong>: Weight the KL term by <InlineMath math="\beta" /> to trade off reconstruction quality vs. disentanglement: <InlineMath math="\mathcal{L} = \text{Recon} + \beta \cdot D_{KL}" />. Higher <InlineMath math="\beta" /> encourages more disentangled latent factors.</li>
          <li><strong>Latent dim matters</strong>: Too few dimensions and the model can&apos;t capture variation. Too many and the KL term pushes unused dimensions to the prior (posterior collapse).</li>
          <li><strong>Use for anomaly detection</strong>: Train a VAE on normal data. At test time, high reconstruction error signals an anomaly.</li>
          <li><strong>VQ-VAE</strong>: Replaces continuous latent space with a discrete codebook. Used in DALL-E 1 and modern audio generation. Avoids posterior collapse entirely.</li>
        </ul>
      </TopicSection>

      <TopicSection type="common-mistakes">
        <ul>
          <li><strong>Outputting logvar but using it as variance</strong>: The encoder outputs <InlineMath math="\log \sigma^2" />, not <InlineMath math="\sigma^2" />. You must exponentiate: <InlineMath math="\sigma = \exp(0.5 \cdot \text{logvar})" />. Forgetting this causes training instability.</li>
          <li><strong>Posterior collapse</strong>: The encoder learns to ignore the input and output the prior <InlineMath math="q(z|x) \approx p(z)" />, while the decoder ignores <InlineMath math="z" />. Fix with KL annealing (gradually increase the KL weight from 0 to 1) or free bits.</li>
          <li><strong>Wrong reconstruction loss for your data</strong>: Use BCE for data in [0, 1] (images with sigmoid decoder). Use MSE for continuous data. Mismatching the loss and decoder causes poor training.</li>
          <li><strong>Summing vs averaging the loss</strong>: The ELBO is typically summed over dimensions and averaged over the batch. Mixing these up changes the effective weight of the KL term.</li>
        </ul>
      </TopicSection>

      <TopicSection type="interview">
        <p><strong>Question:</strong> Derive the ELBO and explain why we need the reparameterization trick. What happens if you just maximize the reconstruction term without the KL term?</p>
        <p><strong>Answer:</strong></p>
        <ol>
          <li><strong>ELBO derivation</strong>:
            <ul>
              <li>Start with <InlineMath math="\log p(x) = \log \int p(x|z)p(z)dz" /></li>
              <li>Multiply and divide by <InlineMath math="q_\phi(z|x)" />: <InlineMath math="\log p(x) = \log E_{q}[p(x|z)p(z)/q(z|x)]" /></li>
              <li>Apply Jensen&apos;s inequality: <InlineMath math="\log p(x) \geq E_q[\log p(x|z)] - D_{KL}(q \| p)" /></li>
              <li>The gap equals <InlineMath math="D_{KL}(q_\phi(z|x) \| p_\theta(z|x))" />, which is non-negative.</li>
            </ul>
          </li>
          <li><strong>Reparameterization trick</strong>:
            <ul>
              <li>We need gradients w.r.t. <InlineMath math="\phi" /> through the expectation <InlineMath math="E_{q_\phi}" />.</li>
              <li>Sampling <InlineMath math="z \sim q_\phi" /> is non-differentiable.</li>
              <li>By writing <InlineMath math="z = \mu_\phi + \sigma_\phi \odot \epsilon" /> with <InlineMath math="\epsilon \sim \mathcal{N}(0,I)" />, the stochasticity is in <InlineMath math="\epsilon" /> (not dependent on <InlineMath math="\phi" />), so we can backprop through <InlineMath math="\mu" /> and <InlineMath math="\sigma" />.</li>
            </ul>
          </li>
          <li><strong>Without KL term</strong>:
            <ul>
              <li>The encoder can map each input to a tiny, isolated region in latent space (effectively memorizing).</li>
              <li>The latent space has no structure — sampling random points produces garbage.</li>
              <li>You get a regular autoencoder, not a generative model.</li>
            </ul>
          </li>
        </ol>
      </TopicSection>

      <TopicSection type="go-deeper">
        <ul>
          <li><strong>Kingma &amp; Welling (2014) &quot;Auto-Encoding Variational Bayes&quot;</strong> — The original VAE paper. Remarkably clear and concise.</li>
          <li><strong>Doersch (2016) &quot;Tutorial on Variational Autoencoders&quot;</strong> — Excellent step-by-step tutorial with intuition and math.</li>
          <li><strong>Van den Oord et al. (2017) &quot;Neural Discrete Representation Learning&quot;</strong> — VQ-VAE, which replaces the continuous latent space with a learned codebook.</li>
          <li><strong>Higgins et al. (2017) &quot;beta-VAE&quot;</strong> — Disentangled representation learning by tuning the KL weight.</li>
        </ul>
      </TopicSection>
    </div>
  );
}
